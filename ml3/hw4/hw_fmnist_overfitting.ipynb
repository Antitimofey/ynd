{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDsVMGiVgSq2"
   },
   "source": [
    "## Переобучение нейронных сетей и борьба с ним\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/girafe_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3isBRG6PgSq6"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "def args_and_kwargs(*args, **kwargs):\n",
    "    return args, kwargs\n",
    "\n",
    "def parse_pytorch_model(model_str):\n",
    "    def parse_layer(layer_str):\n",
    "        layer_name, params = layer_str.split(\"(\", 1)\n",
    "        layer_info = {\"type\": layer_name.strip()}\n",
    "        params_template = layer_str.replace(layer_name, \"args_and_kwargs\")\n",
    "        \n",
    "        param_dict = {}\n",
    "        if len(params):\n",
    "            args, kwargs = eval(params_template)\n",
    "            if len(args) or len(kwargs):\n",
    "                param_dict[\"args\"] = args\n",
    "                for name, value in kwargs.items():\n",
    "                    param_dict[name] = value\n",
    "        layer_info[\"parameters\"] = param_dict\n",
    "        return layer_info\n",
    "\n",
    "    model_dict = {}\n",
    "    lines = model_str.splitlines()\n",
    "    model_name = lines[0].strip(\"()\")\n",
    "    model_dict[\"model_name\"] = model_name\n",
    "    model_dict[\"layers\"] = []\n",
    "\n",
    "    layer_regex = re.compile(r\"\\((\\d+)\\): (.+)\")\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        match = layer_regex.match(line)\n",
    "        if match:\n",
    "            index, layer = match.groups()\n",
    "            model_dict[\"layers\"].append({\"index\": int(index), \"layer\": parse_layer(layer)})\n",
    "    return model_dict\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "\n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def get_accuracy(model, data_loader):\n",
    "    predicted_labels = []\n",
    "    real_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            y_predicted = model(batch[0].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "            real_labels.append(batch[1])\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    real_labels = torch.cat(real_labels)\n",
    "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
    "    return accuracy_score\n",
    "\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-23 18:01:23--  https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict [following]\n",
      "--2025-04-23 18:01:24--  https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6272446 (6.0M) [application/octet-stream]\n",
      "Saving to: ‘hw_overfitting_data_dict.npy’\n",
      "\n",
      "hw_overfitting_data 100%[===================>]   5.98M   995KB/s    in 6.7s    \n",
      "\n",
      "2025-04-23 18:01:31 (919 KB/s) - ‘hw_overfitting_data_dict.npy’ saved [6272446/6272446]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict -O hw_overfitting_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeA6Q5-CgSq7"
   },
   "source": [
    "### Задача №1 (уже решённая): Создание и обучение модели (Separation)\n",
    "Вы уже решали эту задачу ранее, так что сейчас просто воспроизведите своё решение. Оно понадобится вам в дальнейших шагах.\n",
    "__Ваша первая задача всё та же: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE_ID = 0  # change if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nPG1KbQAgl8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = (\n",
    "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# __________end of block__________\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "aYcL28OsgSq8",
    "outputId": "93aafa07-fb56-43bd-f928-918f45fe30e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAALJJJREFUeJzt3Xt0VOW9//HPTC6ThNwIgVwgYIhcvHCpqIhWROEAcXmhcFTU9ROsBbWBClSr6akiaE2LraUq1a5qoT2CWM9BvNTSKtdWAQtKwbYiwSAgBCWSBBJym3l+f3CYdiBcnm2SJwnv11qzFtnZ3+zvPNnwyZ7ZfOMzxhgBANDC/K4bAACcmQggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggoIXt2LFDPp9PCxYssK59+OGH5fP5tH///ibrZ+LEiTrrrLOa7OsBp4sAQquyYMEC+Xw+bdiwwXUrOE01NTUqKirSueeeq4SEBHXt2lU33HCD/v73v7tuDa1ctOsGALRtt956q1577TVNmjRJF1xwgfbs2aN58+ZpyJAh2rJli3r06OG6RbRSBBAAzz777DMtWbJE9957rx5//PHw9ssvv1xXXXWVlixZounTpzvsEK0ZL8Gh1Zs4caISExO1c+dOXXPNNUpMTFTXrl01b948SdKWLVt01VVXqUOHDurRo4cWLVoUUf/ll1/q3nvvVb9+/ZSYmKjk5GTl5+frb3/723HH+vTTT3XdddepQ4cO6tKli6ZPn64//vGP8vl8WrVqVcS+69ev1+jRo5WSkqKEhARdccUVeueddzw9x82bN2vixInq2bOn4uLilJmZqW9+85sqKytrdP/9+/frxhtvVHJysjp16qR77rlHNTU1x+33wgsvaNCgQYqPj1daWprGjx+vXbt2nbKfvXv36qOPPlJ9ff1J9zt48KAkKSMjI2J7VlaWJCk+Pv6Ux8KZiwBCmxAMBpWfn6+cnBzNmTNHZ511lqZMmaIFCxZo9OjRuvDCC/XjH/9YSUlJuu2221RSUhKu/eSTT7R06VJdc801euKJJ3Tfffdpy5YtuuKKK7Rnz57wflVVVbrqqqv09ttv6zvf+Y7+67/+S++++67uv//+4/pZsWKFhg4dqsrKSs2cOVOPPfaYysvLddVVV+m9996zfn5vvfWWPvnkE91+++166qmnNH78eC1evFhXX321GvuNKTfeeGP4vZerr75aTz75pCZPnhyxzw9/+EPddttt6tWrl5544glNmzZNy5cv19ChQ1VeXn7SfgoLC3XOOefos88+O+l+eXl56tatm37605/q9ddf1+7du/Xee+/prrvuUm5ursaPH2+9FjiDGKAVmT9/vpFk/vrXv4a3TZgwwUgyjz32WHjbgQMHTHx8vPH5fGbx4sXh7R999JGRZGbOnBneVlNTY4LBYMRxSkpKTCAQMLNnzw5v++lPf2okmaVLl4a3HT582PTt29dIMitXrjTGGBMKhUyvXr3MqFGjTCgUCu9bXV1tcnNzzX/8x3+c9DmWlJQYSWb+/PkRtcd68cUXjSSzZs2a8LaZM2caSea6666L2Pfb3/62kWT+9re/GWOM2bFjh4mKijI//OEPI/bbsmWLiY6Ojtg+YcIE06NHj4j9jq55SUnJSZ+LMcasX7/e5OXlGUnhx6BBg8zevXtPWYszG1dAaDO+9a1vhf+cmpqqPn36qEOHDrrxxhvD2/v06aPU1FR98skn4W2BQEB+/5FTPRgMqqysTImJierTp4/ef//98H7Lli1T165ddd1114W3xcXFadKkSRF9bNq0Sdu2bdMtt9yisrIy7d+/X/v371dVVZWGDx+uNWvWKBQKWT23f3+pqqamRvv379cll1wiSRE9HlVQUBDx8dSpUyVJb775piRpyZIlCoVCuvHGG8P97d+/X5mZmerVq5dWrlx50n4WLFggY8xp3Z7dsWNHDRw4UA888ICWLl2qn/zkJ9qxY4duuOGGRl8WBI7iJgS0CXFxcercuXPEtpSUFHXr1k0+n++47QcOHAh/HAqF9POf/1y/+MUvVFJSomAwGP5cp06dwn/+9NNPlZeXd9zXO/vssyM+3rZtmyRpwoQJJ+y3oqJCHTt2PM1nd+R9qlmzZmnx4sX6/PPPj/tax+rVq1fEx3l5efL7/dqxY0e4R2PMcfsdFRMTc9q9nUxFRYUuv/xy3Xffffrud78b3n7hhRdq2LBhmj9/vu6+++4mORbaHwIIbUJUVJTVdvNv75s89thjevDBB/XNb35TjzzyiNLS0uT3+zVt2jTrKxVJ4ZrHH39cAwcObHSfxMREq69544036t1339V9992ngQMHKjExUaFQSKNHjz6tHo8NzVAoJJ/Ppz/84Q+NrpFtfyfyv//7v9q3b1/EVaMkXXHFFUpOTtY777xDAOGECCC0e//zP/+jK6+8Us8//3zE9vLycqWnp4c/7tGjh/7xj3/IGBPxD3pxcXFEXV5eniQpOTlZI0aM+Mr9HThwQMuXL9esWbP00EMPhbcfvdJqzLZt25SbmxvRYygUCr9klpeXJ2OMcnNz1bt376/c44ns27dPkiKuKqUjPwAEg0E1NDQ027HR9vEeENq9qKio4+4ke/nll4+7w2vUqFH67LPP9Nprr4W31dTU6Fe/+lXEfoMGDVJeXp5+8pOf6NChQ8cd74svvrDuT9JxPc6dO/eENUdvQT/qqaeekiTl5+dLksaOHauoqCjNmjXruK9rjDnh7d1Hne5t2EfDbfHixRHbX3vtNVVVVelrX/vaSetxZuMKCO3eNddco9mzZ+v222/XpZdeqi1btmjhwoXq2bNnxH533nmnnn76ad1888265557lJWVpYULFyouLk7Sv17m8vv9eu6555Sfn6/zzjtPt99+u7p27arPPvtMK1euVHJysl5//fXT7i85OVlDhw7VnDlzVF9fr65du+pPf/pTxK3kxyopKdF1112n0aNHa+3atXrhhRd0yy23aMCAAZKOXAE9+uijKiws1I4dOzRmzBglJSWppKREr7zyiiZPnqx77733hF+/sLBQv/nNb1RSUnLSGxGuvfZanXfeeZo9e7Y+/fRTXXLJJSouLtbTTz+trKws3XHHHae9DjjzEEBo977//e+rqqpKixYt0ksvvaQLLrhAv//97/XAAw9E7JeYmKgVK1Zo6tSp+vnPf67ExETddtttuvTSSzVu3LhwEEnSsGHDtHbtWj3yyCN6+umndejQIWVmZmrw4MG68847rXtctGiRpk6dqnnz5skYo5EjR+oPf/iDsrOzG93/pZde0kMPPaQHHnhA0dHRmjJlSsQkAkl64IEH1Lt3b/3sZz/TrFmzJEk5OTkaOXLkce/ZeBUbG6s///nPeuSRR/T73/9eL774opKSkjRmzBg99thjES9xAsfymWOvzwFEmDt3rqZPn67du3era9eurtsB2g0CCPg3hw8fPu7/5Hzta19TMBjUxx9/7LAzoP3hJTjg34wdO1bdu3fXwIEDVVFRoRdeeEEfffSRFi5c6Lo1oN0hgIB/M2rUKD333HNauHChgsGgzj33XC1evFg33XST69aAdoeX4AAATvD/gAAAThBAAAAnWt17QKFQSHv27FFSUtJx860AAK2fMUYHDx5UdnZ2eBJ9Y1pdAO3Zs0c5OTmu2wAAfEW7du1St27dTvj5VhdASUlJkqSv62pFq2lGxqP18MXEWteY+rpm6KRx1dddaF1zoHfjE7lPJmi/DFKU/f1CgS+9vYqQMW+9pzprLfUqB/datagG1esvejP87/mJNFsAzZs3T48//rhKS0s1YMAAPfXUU7r44otPWXf0ZbdoxSjaRwC1Nz4P31Pja7l/PKJj4k690zGiAvYBpIB9ifEQQFEBb//At9jfvRZ7mZ0AalH/t9ynehulWW5CeOmllzRjxgzNnDlT77//vgYMGKBRo0Yd94u2AABnrmYJoCeeeEKTJk3S7bffrnPPPVfPPvusEhIS9Otf/7o5DgcAaIOaPIDq6uq0cePGiF/U5ff7NWLECK1du/a4/Wtra1VZWRnxAAC0f00eQPv371cwGFRGRkbE9oyMDJWWlh63f1FRkVJSUsIP7oADgDOD8/+IWlhYqIqKivBj165drlsCALSAJr8LLj09XVFRUeHfFX/Uvn37lJmZedz+gUBAgYCHW4IAAG1ak18BxcbGatCgQVq+fHl4WygU0vLlyzVkyJCmPhwAoI1qlv8HNGPGDE2YMEEXXnihLr74Ys2dO1dVVVW6/fbbm+NwAIA2qFkC6KabbtIXX3yhhx56SKWlpRo4cKCWLVt23I0JAIAzV6v7fUCVlZVKSUnRMF3PJARIkvwDzrGuKb4l1dOxvEwb6P2r/dY1wa3F1jVelE3y9rJ32deC1jXZq+xf0U/83TrrGrR+DaZeq/SqKioqlJycfML9nN8FBwA4MxFAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADAiWaZhg2H/FH2NSH7wZNe7b/TfjjmgfNC1jV9n/7cukaSgh9vt6/xdKSW0elXa73VeagpnnuJdU1qR/vzIf2X3p6TJz6ffU3rmu/cqnEFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACeYht2atfLJ1rt+cKl1jfHwlHp9Z711jddV8CckWNeY+gb7moZ66xr57H9e9MfH2R9HUqiqyrrm7GnrrGtKHrOfhh2can/eZTz1rnWNJG+TrZmgfdq4AgIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJxhG2pq10GDRHY/aD4SUpGj7eZXKedTjUEhbXga5SgpVV9sXtdTwSWN/PngZKiqpxQbh5n5/rXXNjkfsz9c937MfYCpJ2XM8nK8MMD1tXAEBAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMMI/WiFQ8O/Pj5C61rOnzs4flI6vrjFhos6mW9W2iQq6R2MRTyOF7Wr4UGmJ71oP0A0+0/vcS6RpI+fvZi65red71nf6D2eA6dBq6AAABOEEAAACeaPIAefvhh+Xy+iEffvn2b+jAAgDauWd4DOu+88/T222//6yDRvNUEAIjULMkQHR2tzMzM5vjSAIB2olneA9q2bZuys7PVs2dP3Xrrrdq5c+cJ962trVVlZWXEAwDQ/jV5AA0ePFgLFizQsmXL9Mwzz6ikpESXX365Dh482Oj+RUVFSklJCT9ycnKauiUAQCvU5AGUn5+vG264Qf3799eoUaP05ptvqry8XL/73e8a3b+wsFAVFRXhx65du5q6JQBAK9Tsdwekpqaqd+/eKi4ubvTzgUBAgUCgudsAALQyzf7/gA4dOqTt27crKyuruQ8FAGhDmjyA7r33Xq1evVo7duzQu+++q2984xuKiorSzTff3NSHAgC0YU3+Etzu3bt18803q6ysTJ07d9bXv/51rVu3Tp07d27qQwEA2jCfMa1rCl5lZaVSUlI0TNcr2hfjup3GtdDQRS8DFIOpDdY1ve/YYF3jWQutHdqIVjzYV5I+fs5+uK+C9s+p951/tT9OK9Zg6rVKr6qiokLJyckn3I9ZcAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgRLP/Qrp2qYWGYwbT6q1rEv/Zyn+5X0sNFvUy5BL/4vPws6kJeajxMFi0BQeY9v6W/aDeL9/obV0Tld7Juia4v8y6prXhCggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOMA27hUSlptgXheyn/oai7A9zaFlP+yJJez9Pta6J/yjOuqbbY+9a13idfoz/Y1poarkXLfi99Q84x7rmywP253ja/o+ta9oDroAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAmGkXrhsx8SWvxsD+uaxEC1dY3/cKx1TfyPUq1rJCkz0/5YFbn2x6kfeaF1TdzuSvsDSTK79toXhUL2Nf6W+dnPF7D/Hnllqg9b1wQH9rKu2XZHjHVNavoh6xpJqmuw/z75au2Hpfo7dLCuCVVVWde0NlwBAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATDCP1wtgPG7x/wJ+sa+Ztu8K65kC/Ouua+sQ46xpJStxtvw5Ju+wHd1bk2g+f3Dsk3bpGkvz19nXxX9ivQ32C/UDbulTrEkXV2NdIkr/BvuZgnn1RUvZB65pUX611TfWWjtY1kpT0qX1N4NovrWsaLuxtXeNf/YF1TWvDFRAAwAkCCADghHUArVmzRtdee62ys7Pl8/m0dOnSiM8bY/TQQw8pKytL8fHxGjFihLZt29ZU/QIA2gnrAKqqqtKAAQM0b968Rj8/Z84cPfnkk3r22We1fv16dejQQaNGjVJNjccXowEA7ZL1TQj5+fnKz89v9HPGGM2dO1c/+MEPdP3110uSfvvb3yojI0NLly7V+PHjv1q3AIB2o0nfAyopKVFpaalGjBgR3paSkqLBgwdr7dq1jdbU1taqsrIy4gEAaP+aNIBKS0slSRkZGRHbMzIywp87VlFRkVJSUsKPnJycpmwJANBKOb8LrrCwUBUVFeHHrl27XLcEAGgBTRpAmZmZkqR9+/ZFbN+3b1/4c8cKBAJKTk6OeAAA2r8mDaDc3FxlZmZq+fLl4W2VlZVav369hgwZ0pSHAgC0cdZ3wR06dEjFxcXhj0tKSrRp0yalpaWpe/fumjZtmh599FH16tVLubm5evDBB5Wdna0xY8Y0Zd8AgDbOOoA2bNigK6+8MvzxjBkzJEkTJkzQggUL9L3vfU9VVVWaPHmyysvL9fWvf13Lli1TXJy3eWMAgPbJOoCGDRsmc5JhnD6fT7Nnz9bs2bO/UmOtWXRON+uazJiPrGsqKhKsa2L32g/urMkMWtdIkomOsq6JOmw/hNMLf723urgy+8Gifvv5r4qK9nIc+7XzOoy08hz7waIJnausa7IftT+HQrH2NSbG20LsvjLeuibRb/+9Lb3I/jjZq61LWh3nd8EBAM5MBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOGE9DRtSVf8s65pOfvtJwSZo//NByH4YtqKqvf0cUptmP0Xby0TnQJl9f7GV1iWSpOhq+xovax5baT8xOX5/yLomusb+OJLUsdi+LqrG/leulPe1X7zKs+zPh8PZ9tO9JSm7Z6l1zf6KROuaOG/ttXlcAQEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwwj9eBwmv2yxfjsB3eaBvvBnaFY+yGSHlrzzHg442rT7Ydw1mTYH0eSog7br3lUjX1NTaaHRY/28L2t9fozpv1zUrL9c0pJKbeuyUg4bF3TKc5+GLBXPZIOWNd8tr1XM3TS+nEFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOMIzUg4Z4+5o/V/e2roktjbGuqUu3HwgZXRllXSNJPvsZoZL9PE0ZLz8meTiO5G0wa0O8hyGhHgbNKmhf46/1cBxJwUT7hTi76xfWNSmx9oNFY/32vX1Rk2hdI0l+DyfSuOz3rWt+md3XuibOuqL14QoIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJxgGKkHDQn2Ax4T/HX2B/IyUDPay4RQb8NIW4qXAaHyNoNTdZn11jUxifbfW3/Qfs2Nh29tyHhbiJjYBuuasqoE65qGkP3PwFV1sdY1fp+36bRnpXxpXbPmgP3g4S8vtF/v9F9al7Q6XAEBAJwggAAATlgH0Jo1a3TttdcqOztbPp9PS5cujfj8xIkT5fP5Ih6jR49uqn4BAO2EdQBVVVVpwIABmjdv3gn3GT16tPbu3Rt+vPjii1+pSQBA+2N9E0J+fr7y8/NPuk8gEFBmZqbnpgAA7V+zvAe0atUqdenSRX369NHdd9+tsrKyE+5bW1urysrKiAcAoP1r8gAaPXq0fvvb32r58uX68Y9/rNWrVys/P1/BYOP30hYVFSklJSX8yMnJaeqWAACtUJP/P6Dx48eH/9yvXz/1799feXl5WrVqlYYPH37c/oWFhZoxY0b448rKSkIIAM4AzX4bds+ePZWenq7i4uJGPx8IBJScnBzxAAC0f80eQLt371ZZWZmysrKa+1AAgDbE+iW4Q4cORVzNlJSUaNOmTUpLS1NaWppmzZqlcePGKTMzU9u3b9f3vvc9nX322Ro1alSTNg4AaNusA2jDhg268sorwx8fff9mwoQJeuaZZ7R582b95je/UXl5ubKzszVy5Eg98sgjCgQCTdc1AKDNsw6gYcOGyZgTD/b74x//+JUaagsa4u1rMqPL7Y+T6GGAYr2HV1W9zWmU8dsX+kIehmN6KAl19jD8VZLq7NevvsL+hytfwH6y6En+2jW5+ir7gZ8HDsdY15RHdbCuie9Qa13Tp/Pn1jWSFB9lP5x2x8E065rE9CrrmvaAWXAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwosl/JfeZoD7Zfizxp3Wd7Q/kYQp09MEo+8ME7Y8jydOPL14maIc62E+OVq39OkiSfC0zctp4mFruC9h/o7xMjpakLsmHrGti/fb9JcXWWNd0CthPjs4MVFrXSFKMh78c/5n+V+uaFRXnWtd8aF3R+nAFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOMIzUCw/zKhP89kMh/TX200gb0uuta2JLY6xrJMlXb99fKMF+uGNUQoN1jVcJHeyHYybE2q95tN9+wGpd0H7Aakqc/fORpMP19udEvYf+OsZVW9fUhez/2fJ7+Usr6WAwzrrmqZ3DrWt6JH5pXSMd9lDTunAFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOMIzUg4Yk+0GSl8Z/Yl3jsz+MfNX2AyGNxx9DTIyHAY+x9k/KeFiHhET74a+SVFNjP4Sztta+xu+3X7uYGPuhrMbEW9dIUofYOuua3OQy6xovQ0Kj/fYDbbdUZlvXSFKnQJV1TUK0/dq9ta6/dU0vrbeuaW24AgIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJxhG6oFJtB8KeU5sgnVNdO+D1jX1B+yHTzZ4HUYa5WEYachnf5ygfYOHyr0N4YwO2A+6DAbtn1Ncgv3AyqQ4+wGrh+vsB6VKUiDK/hzfW51sXRMbZb/esX773ryK8tmf40vOfsu65vKf3Wld0x5wBQQAcIIAAgA4YRVARUVFuuiii5SUlKQuXbpozJgx2rp1a8Q+NTU1KigoUKdOnZSYmKhx48Zp3759Tdo0AKDtswqg1atXq6CgQOvWrdNbb72l+vp6jRw5UlVV//qlTdOnT9frr7+ul19+WatXr9aePXs0duzYJm8cANC2Wd2EsGzZsoiPFyxYoC5dumjjxo0aOnSoKioq9Pzzz2vRokW66qqrJEnz58/XOeeco3Xr1umSSy5pus4BAG3aV3oPqKKiQpKUlpYmSdq4caPq6+s1YsSI8D59+/ZV9+7dtXbt2ka/Rm1trSorKyMeAID2z3MAhUIhTZs2TZdddpnOP/98SVJpaaliY2OVmpoasW9GRoZKS0sb/TpFRUVKSUkJP3Jycry2BABoQzwHUEFBgT788EMtXrz4KzVQWFioioqK8GPXrl1f6esBANoGT/8RdcqUKXrjjTe0Zs0adevWLbw9MzNTdXV1Ki8vj7gK2rdvnzIzMxv9WoFAQIFAwEsbAIA2zOoKyBijKVOm6JVXXtGKFSuUm5sb8flBgwYpJiZGy5cvD2/bunWrdu7cqSFDhjRNxwCAdsHqCqigoECLFi3Sq6++qqSkpPD7OikpKYqPj1dKSoruuOMOzZgxQ2lpaUpOTtbUqVM1ZMgQ7oADAESwCqBnnnlGkjRs2LCI7fPnz9fEiRMlST/72c/k9/s1btw41dbWatSoUfrFL37RJM0CANoPqwAy5tSD+eLi4jRv3jzNmzfPc1OtXfab9m+d1Y+0H7rYt4v9BIlNe3pa15gYD0NFJclLmZcav32Rz8PQU8lbe9Gx9t/b+voo6xovg0WTPQwwlaQOMfZ1B+vjrGuiffZDWaP9Iesar6J99t/bx7/Ms65JWvmRdY19Z60Ps+AAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADghKffiHqmS3n7Y+ua3Q2HrWv6JtlPw/4g/izrGjV4mxwdfch+orNkX9PQscH+MNHeJiYHD9v/leiYUWldkxpvfz7ER9db14SMt+9tdUOsdY2X/lJia6xrDgftp4J3CRy0rpGkTjFV1jU9Y7+wrnm7PMm6pj3gCggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnGAYqQfBAwesa67ZcKd1zX+evcm6Jja51rqmrjxgXSNJDUlBT3XWgvYDNRPT7Yd9StKAjD3WNTF++3XYXZVqXVN6yH5gpfE4jLRHypfWNXUh+39OvKxDRrz9YNGqBm/neMeYauuaOY/cal2TqrXWNe0BV0AAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4ATDSFtI99t2WNf89zODrWumD1puXZMdYz9cVZI+PNzNuibOX29dk+Cvs67ZXdfRukaS/rSzr3XNwIzPrGum9XjLumZXXSfrml9uu9y6RpL2ViVb18RG2Q9lTYq1H57rZfhrVTDWukaS1pedZV2T+t9n5mBRL7gCAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnGEbaQkJVVdY1iX+Ls65Zf3audU2H6GzrGkkK+Bs81dnaczjFusbLME1J8vmMdc3qv55rXbOjb5p1TUbCQeua9ET7806Syg/HW9ckxtgPje2WUG5ds7PKftDseSl7rWskqfjXQ6xrOsl+OO2ZiisgAIATBBAAwAmrACoqKtJFF12kpKQkdenSRWPGjNHWrVsj9hk2bJh8Pl/E46677mrSpgEAbZ9VAK1evVoFBQVat26d3nrrLdXX12vkyJGqOub9jUmTJmnv3r3hx5w5c5q0aQBA22d1E8KyZcsiPl6wYIG6dOmijRs3aujQoeHtCQkJyszMbJoOAQDt0ld6D6iiokKSlJYWeUfPwoULlZ6ervPPP1+FhYWqrq4+4deora1VZWVlxAMA0P55vg07FApp2rRpuuyyy3T++eeHt99yyy3q0aOHsrOztXnzZt1///3aunWrlixZ0ujXKSoq0qxZs7y2AQBoozwHUEFBgT788EP95S9/idg+efLk8J/79eunrKwsDR8+XNu3b1deXt5xX6ewsFAzZswIf1xZWamcnByvbQEA2ghPATRlyhS98cYbWrNmjbp163bSfQcPHixJKi4ubjSAAoGAAoGAlzYAAG2YVQAZYzR16lS98sorWrVqlXJzT/2/7jdt2iRJysrK8tQgAKB9sgqggoICLVq0SK+++qqSkpJUWloqSUpJSVF8fLy2b9+uRYsW6eqrr1anTp20efNmTZ8+XUOHDlX//v2b5QkAANomqwB65plnJB35z6b/bv78+Zo4caJiY2P19ttva+7cuaqqqlJOTo7GjRunH/zgB03WMACgfbB+Ce5kcnJytHr16q/UEADgzMA07FbsUG7QuuadLb2sawKlMdY1ktRhj/3k6Oosn3VNTY9a65rogP3aSVJe5hfWNfG96q1rftNnoXVN0H659efDZ9kXSRqbuNu65rmKvtY1v90+2LomPtZ+vf8ub+9Bd3p+rac6nB6GkQIAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwwjbcV6zz9kXfPZlSnWNTWdPUy5lFR5/C+4PSVfd/vnlJFcbV1T1xBlXSNJiTEeBp/6Q9Y1/69gxql3OkbCzoPWNb56b0NZF9c3WNcEt31iXdNZW61r/AkJ1jX11fbnEJofV0AAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMCJVjcLzpgjc8kaVC95G1HWbviC9nPJgrU11jWhGo8L7fNQUm3fXzDKwzoEvc2Cq4+ts68J2q9fQ739OjR4OB98QW+z4BT0MAvO1Hs7liW/sf8ehVqoNxzRoCPrffTf8xPxmVPt0cJ2796tnJwc120AAL6iXbt2qVu3bif8fKsLoFAopD179igpKUk+X+SP2JWVlcrJydGuXbuUnJzsqEP3WIcjWIcjWIcjWIcjWsM6GGN08OBBZWdny+8/8Ts9re4lOL/ff9LElKTk5OQz+gQ7inU4gnU4gnU4gnU4wvU6pKSc+lfDcBMCAMAJAggA4ESbCqBAIKCZM2cqEAi4bsUp1uEI1uEI1uEI1uGItrQOre4mBADAmaFNXQEBANoPAggA4AQBBABwggACADhBAAEAnGgzATRv3jydddZZiouL0+DBg/Xee++5bqnFPfzww/L5fBGPvn37um6r2a1Zs0bXXnutsrOz5fP5tHTp0ojPG2P00EMPKSsrS/Hx8RoxYoS2bdvmptlmdKp1mDhx4nHnx+jRo90020yKiop00UUXKSkpSV26dNGYMWO0devWiH1qampUUFCgTp06KTExUePGjdO+ffscddw8Tmcdhg0bdtz5cNdddznquHFtIoBeeuklzZgxQzNnztT777+vAQMGaNSoUfr8889dt9bizjvvPO3duzf8+Mtf/uK6pWZXVVWlAQMGaN68eY1+fs6cOXryySf17LPPav369erQoYNGjRqlmhr7idOt2anWQZJGjx4dcX68+OKLLdhh81u9erUKCgq0bt06vfXWW6qvr9fIkSNVVVUV3mf69Ol6/fXX9fLLL2v16tXas2ePxo4d67Drpnc66yBJkyZNijgf5syZ46jjEzBtwMUXX2wKCgrCHweDQZOdnW2KioocdtXyZs6caQYMGOC6DackmVdeeSX8cSgUMpmZmebxxx8PbysvLzeBQMC8+OKLDjpsGceugzHGTJgwwVx//fVO+nHl888/N5LM6tWrjTFHvvcxMTHm5ZdfDu/zz3/+00gya9euddVmszt2HYwx5oorrjD33HOPu6ZOQ6u/Aqqrq9PGjRs1YsSI8Da/368RI0Zo7dq1DjtzY9u2bcrOzlbPnj116623aufOna5bcqqkpESlpaUR50dKSooGDx58Rp4fq1atUpcuXdSnTx/dfffdKisrc91Ss6qoqJAkpaWlSZI2btyo+vr6iPOhb9++6t69e7s+H45dh6MWLlyo9PR0nX/++SosLFR1dbWL9k6o1U3DPtb+/fsVDAaVkZERsT0jI0MfffSRo67cGDx4sBYsWKA+ffpo7969mjVrli6//HJ9+OGHSkpKct2eE6WlpZLU6Plx9HNnitGjR2vs2LHKzc3V9u3b9f3vf1/5+flau3atoqK8/YK+1iwUCmnatGm67LLLdP7550s6cj7ExsYqNTU1Yt/2fD40tg6SdMstt6hHjx7Kzs7W5s2bdf/992vr1q1asmSJw24jtfoAwr/k5+eH/9y/f38NHjxYPXr00O9+9zvdcccdDjtDazB+/Pjwn/v166f+/fsrLy9Pq1at0vDhwx121jwKCgr04YcfnhHvg57MidZh8uTJ4T/369dPWVlZGj58uLZv3668vLyWbrNRrf4luPT0dEVFRR13F8u+ffuUmZnpqKvWITU1Vb1791ZxcbHrVpw5eg5wfhyvZ8+eSk9Pb5fnx5QpU/TGG29o5cqVEb8/LDMzU3V1dSovL4/Yv72eDydah8YMHjxYklrV+dDqAyg2NlaDBg3S8uXLw9tCoZCWL1+uIUOGOOzMvUOHDmn79u3Kyspy3Yozubm5yszMjDg/KisrtX79+jP+/Ni9e7fKysra1flhjNGUKVP0yiuvaMWKFcrNzY34/KBBgxQTExNxPmzdulU7d+5sV+fDqdahMZs2bZKk1nU+uL4L4nQsXrzYBAIBs2DBAvOPf/zDTJ482aSmpprS0lLXrbWo7373u2bVqlWmpKTEvPPOO2bEiBEmPT3dfP75565ba1YHDx40H3zwgfnggw+MJPPEE0+YDz74wHz66afGGGN+9KMfmdTUVPPqq6+azZs3m+uvv97k5uaaw4cPO+68aZ1sHQ4ePGjuvfdes3btWlNSUmLefvttc8EFF5hevXqZmpoa1603mbvvvtukpKSYVatWmb1794Yf1dXV4X3uuusu0717d7NixQqzYcMGM2TIEDNkyBCHXTe9U61DcXGxmT17ttmwYYMpKSkxr776qunZs6cZOnSo484jtYkAMsaYp556ynTv3t3Exsaaiy++2Kxbt851Sy3upptuMllZWSY2NtZ07drV3HTTTaa4uNh1W81u5cqVRtJxjwkTJhhjjtyK/eCDD5qMjAwTCATM8OHDzdatW9023QxOtg7V1dVm5MiRpnPnziYmJsb06NHDTJo0qd39kNbY85dk5s+fH97n8OHD5tvf/rbp2LGjSUhIMN/4xjfM3r173TXdDE61Djt37jRDhw41aWlpJhAImLPPPtvcd999pqKiwm3jx+D3AQEAnGj17wEBANonAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABw4v8Dueyy5xdcONEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_fmnist_data = FashionMNIST(\n",
    "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_fmnist_data = FashionMNIST(\n",
    "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f\"Image label: {_label}\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6jWRv1rgSq8"
   },
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BcyEFX-RgSq8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    (6): Linear(in_features=3136, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating model instance\n",
    "model_task_1 = None\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #nn.Linear(28*28, 512)\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_task_1 = NeuralNetwork().to(device)\n",
    "display(model_task_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAoLV4dkoy5M"
   },
   "source": [
    "Не забудьте перенести модель на выбранный `device`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Xas9SIXDoxvZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    (6): Linear(in_features=3136, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task_1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pLRWysggSq9"
   },
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qMQzo1ggSq9",
    "outputId": "c00008eb-ef88-4000-ce47-e8dedd26e061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].to(device)\n",
    "    y = random_batch[1].to(device)\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model_task_1(x)\n",
    "except Exception as e:\n",
    "    print(\"Something is wrong with the model\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
    "\n",
    "print(\"Everything seems fine!\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suRmIPwIgSq9"
   },
   "source": [
    "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YJnU14bdnZa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.494677  [   32/60000]\n",
      "loss: 0.529336  [ 3232/60000]\n",
      "loss: 0.647709  [ 6432/60000]\n",
      "loss: 0.332088  [ 9632/60000]\n",
      "loss: 0.673110  [12832/60000]\n",
      "loss: 0.860064  [16032/60000]\n",
      "loss: 0.492295  [19232/60000]\n",
      "loss: 0.526473  [22432/60000]\n",
      "loss: 0.635597  [25632/60000]\n",
      "loss: 0.545944  [28832/60000]\n",
      "loss: 0.325143  [32032/60000]\n",
      "loss: 0.559311  [35232/60000]\n",
      "loss: 0.527919  [38432/60000]\n",
      "loss: 0.374021  [41632/60000]\n",
      "loss: 0.408359  [44832/60000]\n",
      "loss: 0.199731  [48032/60000]\n",
      "loss: 0.388632  [51232/60000]\n",
      "loss: 0.438638  [54432/60000]\n",
      "loss: 0.383907  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.431914 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.262233  [   32/60000]\n",
      "loss: 0.244267  [ 3232/60000]\n",
      "loss: 0.363155  [ 6432/60000]\n",
      "loss: 0.309728  [ 9632/60000]\n",
      "loss: 0.300173  [12832/60000]\n",
      "loss: 0.167306  [16032/60000]\n",
      "loss: 0.310073  [19232/60000]\n",
      "loss: 0.550868  [22432/60000]\n",
      "loss: 0.567805  [25632/60000]\n",
      "loss: 0.322740  [28832/60000]\n",
      "loss: 0.334151  [32032/60000]\n",
      "loss: 0.289611  [35232/60000]\n",
      "loss: 0.261225  [38432/60000]\n",
      "loss: 0.358007  [41632/60000]\n",
      "loss: 0.295868  [44832/60000]\n",
      "loss: 0.275837  [48032/60000]\n",
      "loss: 0.347429  [51232/60000]\n",
      "loss: 0.698200  [54432/60000]\n",
      "loss: 0.540518  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.394148 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.210321  [   32/60000]\n",
      "loss: 0.422335  [ 3232/60000]\n",
      "loss: 0.324559  [ 6432/60000]\n",
      "loss: 0.324305  [ 9632/60000]\n",
      "loss: 0.582471  [12832/60000]\n",
      "loss: 0.501622  [16032/60000]\n",
      "loss: 0.215590  [19232/60000]\n",
      "loss: 0.255207  [22432/60000]\n",
      "loss: 0.403817  [25632/60000]\n",
      "loss: 0.374497  [28832/60000]\n",
      "loss: 0.244852  [32032/60000]\n",
      "loss: 0.375775  [35232/60000]\n",
      "loss: 0.366733  [38432/60000]\n",
      "loss: 0.292638  [41632/60000]\n",
      "loss: 0.379263  [44832/60000]\n",
      "loss: 0.279612  [48032/60000]\n",
      "loss: 0.383932  [51232/60000]\n",
      "loss: 0.371238  [54432/60000]\n",
      "loss: 0.104186  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.380031 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.517429  [   32/60000]\n",
      "loss: 0.216501  [ 3232/60000]\n",
      "loss: 0.246338  [ 6432/60000]\n",
      "loss: 0.301049  [ 9632/60000]\n",
      "loss: 0.258725  [12832/60000]\n",
      "loss: 0.399209  [16032/60000]\n",
      "loss: 0.403798  [19232/60000]\n",
      "loss: 0.291013  [22432/60000]\n",
      "loss: 0.705244  [25632/60000]\n",
      "loss: 0.600833  [28832/60000]\n",
      "loss: 0.197910  [32032/60000]\n",
      "loss: 0.421941  [35232/60000]\n",
      "loss: 0.463194  [38432/60000]\n",
      "loss: 0.091221  [41632/60000]\n",
      "loss: 0.665212  [44832/60000]\n",
      "loss: 0.185679  [48032/60000]\n",
      "loss: 0.354114  [51232/60000]\n",
      "loss: 0.323862  [54432/60000]\n",
      "loss: 0.323631  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.353470 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.379261  [   32/60000]\n",
      "loss: 0.412657  [ 3232/60000]\n",
      "loss: 0.277952  [ 6432/60000]\n",
      "loss: 0.325568  [ 9632/60000]\n",
      "loss: 0.458668  [12832/60000]\n",
      "loss: 0.355357  [16032/60000]\n",
      "loss: 0.365362  [19232/60000]\n",
      "loss: 0.506408  [22432/60000]\n",
      "loss: 0.380393  [25632/60000]\n",
      "loss: 0.456006  [28832/60000]\n",
      "loss: 0.344121  [32032/60000]\n",
      "loss: 0.314262  [35232/60000]\n",
      "loss: 0.150846  [38432/60000]\n",
      "loss: 0.478552  [41632/60000]\n",
      "loss: 0.316730  [44832/60000]\n",
      "loss: 0.178948  [48032/60000]\n",
      "loss: 0.269647  [51232/60000]\n",
      "loss: 0.227625  [54432/60000]\n",
      "loss: 0.280074  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.340178 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import torch.optim.sgd\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_task_1.parameters(), lr=0.01)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "print(1e-1)\n",
    "epochs = 5 # 20 -> 0,91 0,883\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_data_loader, model_task_1, loss_fn, optimizer)\n",
    "    test(test_data_loader, model_task_1, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zce7gt1gSq-"
   },
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usswrWYOgSq-"
   },
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Xua3TVZHgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.8922\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l9KEKXBxgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.8822\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyhmMobgSq_"
   },
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "OAIrURCEgSq_",
    "outputId": "7c983690-a92e-4693-89fb-7c86c002921a"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Train accuracy is below 0.885 threshold",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_acc_task_1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.885\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain accuracy is below 0.885 threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     train_acc_task_1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.905\u001b[39m\n\u001b[1;32m      4\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Train accuracy is below 0.885 threshold"
     ]
    }
   ],
   "source": [
    "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
    "assert (\n",
    "    train_acc_task_1 >= 0.905\n",
    "), \"Test accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_task_1.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    \"train_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "    ),\n",
    "    \"test_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "    ),\n",
    "    \"model_task_1\": parse_pytorch_model(str(model_task_1)),\n",
    "}\n",
    "\n",
    "with open(\"submission_dict_task_1.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_task_1.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №2: Переобучение (Initiation)\n",
    "Продолжим работу с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). Теперь ваша задача продемонстрировать переобучение модели на обучающей выборке. Достаточно показать, что точность классификации (не только функция потерь!) на тестовой выборке значительно отстает от обучающей.\n",
    "\n",
    "Обращаем ваше внимание, в задаче №3 вам придется починить данную модель (минимизировать эффект переобучения) с помощью механизмов регуляризации, поэтому не переусердствуйте!\n",
    "\n",
    "__Ваша вторая задача: реализовать используя пайплан обучения модели продемонстрировать переобучения модели на обучающей выборке.__\n",
    "\n",
    "Код для обучения модели вы можете переиспользовать. Далее присутствует лишь несколько тестов, которые помогут вам проверить свое решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимание, вам необходимо использовать переменную `model_task_2` для хранение модели во второй задаче. \n",
    "\n",
    "Не используйте `Dropout` и `BatchNorm` в этой задаче"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    (6): Linear(in_features=50176, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating model instance\n",
    "model_task_2 = None\n",
    "# Creating model instance\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*28*28, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_task_2 = NeuralNetwork().to(device)\n",
    "display(model_task_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# your code here\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305094  [   32/60000]\n",
      "loss: 1.228994  [ 3232/60000]\n",
      "loss: 1.045748  [ 6432/60000]\n",
      "loss: 0.742443  [ 9632/60000]\n",
      "loss: 0.578746  [12832/60000]\n",
      "loss: 0.839088  [16032/60000]\n",
      "loss: 0.549800  [19232/60000]\n",
      "loss: 1.018300  [22432/60000]\n",
      "loss: 0.317950  [25632/60000]\n",
      "loss: 0.467721  [28832/60000]\n",
      "loss: 0.309594  [32032/60000]\n",
      "loss: 0.774118  [35232/60000]\n",
      "loss: 0.735625  [38432/60000]\n",
      "loss: 0.812112  [41632/60000]\n",
      "loss: 0.388908  [44832/60000]\n",
      "loss: 0.693273  [48032/60000]\n",
      "loss: 0.252064  [51232/60000]\n",
      "loss: 0.323002  [54432/60000]\n",
      "loss: 0.305611  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.447688 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.631908  [   32/60000]\n",
      "loss: 0.549481  [ 3232/60000]\n",
      "loss: 0.424621  [ 6432/60000]\n",
      "loss: 0.604051  [ 9632/60000]\n",
      "loss: 0.398603  [12832/60000]\n",
      "loss: 0.274520  [16032/60000]\n",
      "loss: 0.464481  [19232/60000]\n",
      "loss: 0.481372  [22432/60000]\n",
      "loss: 0.362464  [25632/60000]\n",
      "loss: 0.371421  [28832/60000]\n",
      "loss: 0.460689  [32032/60000]\n",
      "loss: 0.627504  [35232/60000]\n",
      "loss: 0.330879  [38432/60000]\n",
      "loss: 0.460357  [41632/60000]\n",
      "loss: 0.245705  [44832/60000]\n",
      "loss: 0.362684  [48032/60000]\n",
      "loss: 0.617575  [51232/60000]\n",
      "loss: 0.582660  [54432/60000]\n",
      "loss: 0.485056  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.386555 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.501805  [   32/60000]\n",
      "loss: 0.223822  [ 3232/60000]\n",
      "loss: 0.348018  [ 6432/60000]\n",
      "loss: 0.234669  [ 9632/60000]\n",
      "loss: 0.417536  [12832/60000]\n",
      "loss: 0.311156  [16032/60000]\n",
      "loss: 0.436863  [19232/60000]\n",
      "loss: 0.320681  [22432/60000]\n",
      "loss: 0.353501  [25632/60000]\n",
      "loss: 0.508660  [28832/60000]\n",
      "loss: 0.276472  [32032/60000]\n",
      "loss: 0.265333  [35232/60000]\n",
      "loss: 0.425829  [38432/60000]\n",
      "loss: 0.450181  [41632/60000]\n",
      "loss: 0.452336  [44832/60000]\n",
      "loss: 0.377084  [48032/60000]\n",
      "loss: 0.356625  [51232/60000]\n",
      "loss: 0.452492  [54432/60000]\n",
      "loss: 0.475804  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.374033 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.242536  [   32/60000]\n",
      "loss: 0.289802  [ 3232/60000]\n",
      "loss: 0.348339  [ 6432/60000]\n",
      "loss: 0.444955  [ 9632/60000]\n",
      "loss: 0.332877  [12832/60000]\n",
      "loss: 0.376578  [16032/60000]\n",
      "loss: 0.431173  [19232/60000]\n",
      "loss: 0.503596  [22432/60000]\n",
      "loss: 0.473881  [25632/60000]\n",
      "loss: 0.214320  [28832/60000]\n",
      "loss: 0.261588  [32032/60000]\n",
      "loss: 0.278263  [35232/60000]\n",
      "loss: 0.204414  [38432/60000]\n",
      "loss: 0.160095  [41632/60000]\n",
      "loss: 0.501211  [44832/60000]\n",
      "loss: 0.280595  [48032/60000]\n",
      "loss: 0.495271  [51232/60000]\n",
      "loss: 0.351807  [54432/60000]\n",
      "loss: 0.191262  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.346719 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.277415  [   32/60000]\n",
      "loss: 0.269253  [ 3232/60000]\n",
      "loss: 0.253105  [ 6432/60000]\n",
      "loss: 0.298295  [ 9632/60000]\n",
      "loss: 0.434090  [12832/60000]\n",
      "loss: 0.472870  [16032/60000]\n",
      "loss: 0.393618  [19232/60000]\n",
      "loss: 0.165902  [22432/60000]\n",
      "loss: 0.376772  [25632/60000]\n",
      "loss: 0.350306  [28832/60000]\n",
      "loss: 0.288858  [32032/60000]\n",
      "loss: 0.405740  [35232/60000]\n",
      "loss: 0.463393  [38432/60000]\n",
      "loss: 0.412494  [41632/60000]\n",
      "loss: 0.190533  [44832/60000]\n",
      "loss: 0.529869  [48032/60000]\n",
      "loss: 0.219123  [51232/60000]\n",
      "loss: 0.329316  [54432/60000]\n",
      "loss: 0.332544  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.330548 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.157691  [   32/60000]\n",
      "loss: 0.175272  [ 3232/60000]\n",
      "loss: 0.530180  [ 6432/60000]\n",
      "loss: 0.203660  [ 9632/60000]\n",
      "loss: 0.357816  [12832/60000]\n",
      "loss: 0.440407  [16032/60000]\n",
      "loss: 0.212881  [19232/60000]\n",
      "loss: 0.356065  [22432/60000]\n",
      "loss: 0.088906  [25632/60000]\n",
      "loss: 0.357557  [28832/60000]\n",
      "loss: 0.473436  [32032/60000]\n",
      "loss: 0.244281  [35232/60000]\n",
      "loss: 0.159411  [38432/60000]\n",
      "loss: 0.372221  [41632/60000]\n",
      "loss: 0.634482  [44832/60000]\n",
      "loss: 0.327711  [48032/60000]\n",
      "loss: 0.765822  [51232/60000]\n",
      "loss: 0.322703  [54432/60000]\n",
      "loss: 0.210949  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.309733 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.259515  [   32/60000]\n",
      "loss: 0.155454  [ 3232/60000]\n",
      "loss: 0.123614  [ 6432/60000]\n",
      "loss: 0.190019  [ 9632/60000]\n",
      "loss: 0.619316  [12832/60000]\n",
      "loss: 0.441862  [16032/60000]\n",
      "loss: 0.348680  [19232/60000]\n",
      "loss: 0.802846  [22432/60000]\n",
      "loss: 0.115692  [25632/60000]\n",
      "loss: 0.454986  [28832/60000]\n",
      "loss: 0.216838  [32032/60000]\n",
      "loss: 0.200407  [35232/60000]\n",
      "loss: 0.170277  [38432/60000]\n",
      "loss: 0.194609  [41632/60000]\n",
      "loss: 0.231308  [44832/60000]\n",
      "loss: 0.305752  [48032/60000]\n",
      "loss: 0.290812  [51232/60000]\n",
      "loss: 0.207728  [54432/60000]\n",
      "loss: 0.216660  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.294272 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.215164  [   32/60000]\n",
      "loss: 0.248164  [ 3232/60000]\n",
      "loss: 0.470744  [ 6432/60000]\n",
      "loss: 0.161420  [ 9632/60000]\n",
      "loss: 0.220221  [12832/60000]\n",
      "loss: 0.123892  [16032/60000]\n",
      "loss: 0.152755  [19232/60000]\n",
      "loss: 0.194005  [22432/60000]\n",
      "loss: 0.124383  [25632/60000]\n",
      "loss: 0.223231  [28832/60000]\n",
      "loss: 0.276764  [32032/60000]\n",
      "loss: 0.147467  [35232/60000]\n",
      "loss: 0.286484  [38432/60000]\n",
      "loss: 0.078107  [41632/60000]\n",
      "loss: 0.278374  [44832/60000]\n",
      "loss: 0.176526  [48032/60000]\n",
      "loss: 0.202100  [51232/60000]\n",
      "loss: 0.376460  [54432/60000]\n",
      "loss: 0.241465  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.284770 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.209728  [   32/60000]\n",
      "loss: 0.311418  [ 3232/60000]\n",
      "loss: 0.206526  [ 6432/60000]\n",
      "loss: 0.414347  [ 9632/60000]\n",
      "loss: 0.292869  [12832/60000]\n",
      "loss: 0.232529  [16032/60000]\n",
      "loss: 0.378546  [19232/60000]\n",
      "loss: 0.056569  [22432/60000]\n",
      "loss: 0.295093  [25632/60000]\n",
      "loss: 0.119280  [28832/60000]\n",
      "loss: 0.182814  [32032/60000]\n",
      "loss: 0.163735  [35232/60000]\n",
      "loss: 0.277041  [38432/60000]\n",
      "loss: 0.251058  [41632/60000]\n",
      "loss: 0.305520  [44832/60000]\n",
      "loss: 0.297227  [48032/60000]\n",
      "loss: 0.328533  [51232/60000]\n",
      "loss: 0.255969  [54432/60000]\n",
      "loss: 0.118952  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.295841 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.159372  [   32/60000]\n",
      "loss: 0.288734  [ 3232/60000]\n",
      "loss: 0.273724  [ 6432/60000]\n",
      "loss: 0.222375  [ 9632/60000]\n",
      "loss: 0.030994  [12832/60000]\n",
      "loss: 0.098108  [16032/60000]\n",
      "loss: 0.253286  [19232/60000]\n",
      "loss: 0.101507  [22432/60000]\n",
      "loss: 0.338791  [25632/60000]\n",
      "loss: 0.234061  [28832/60000]\n",
      "loss: 0.332277  [32032/60000]\n",
      "loss: 0.241350  [35232/60000]\n",
      "loss: 0.150849  [38432/60000]\n",
      "loss: 0.088690  [41632/60000]\n",
      "loss: 0.053662  [44832/60000]\n",
      "loss: 0.259857  [48032/60000]\n",
      "loss: 0.189399  [51232/60000]\n",
      "loss: 0.059573  [54432/60000]\n",
      "loss: 0.230563  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.319637 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.213938  [   32/60000]\n",
      "loss: 0.165633  [ 3232/60000]\n",
      "loss: 0.076834  [ 6432/60000]\n",
      "loss: 0.094680  [ 9632/60000]\n",
      "loss: 0.106867  [12832/60000]\n",
      "loss: 0.123469  [16032/60000]\n",
      "loss: 0.235543  [19232/60000]\n",
      "loss: 0.396904  [22432/60000]\n",
      "loss: 0.289654  [25632/60000]\n",
      "loss: 0.117256  [28832/60000]\n",
      "loss: 0.426975  [32032/60000]\n",
      "loss: 0.257626  [35232/60000]\n",
      "loss: 0.084134  [38432/60000]\n",
      "loss: 0.286367  [41632/60000]\n",
      "loss: 0.112579  [44832/60000]\n",
      "loss: 0.200263  [48032/60000]\n",
      "loss: 0.436223  [51232/60000]\n",
      "loss: 0.218354  [54432/60000]\n",
      "loss: 0.220079  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.295397 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.205881  [   32/60000]\n",
      "loss: 0.228988  [ 3232/60000]\n",
      "loss: 0.122347  [ 6432/60000]\n",
      "loss: 0.057691  [ 9632/60000]\n",
      "loss: 0.159857  [12832/60000]\n",
      "loss: 0.099883  [16032/60000]\n",
      "loss: 0.069296  [19232/60000]\n",
      "loss: 0.269247  [22432/60000]\n",
      "loss: 0.206644  [25632/60000]\n",
      "loss: 0.368790  [28832/60000]\n",
      "loss: 0.197388  [32032/60000]\n",
      "loss: 0.342506  [35232/60000]\n",
      "loss: 0.112077  [38432/60000]\n",
      "loss: 0.057772  [41632/60000]\n",
      "loss: 0.237182  [44832/60000]\n",
      "loss: 0.120936  [48032/60000]\n",
      "loss: 0.098662  [51232/60000]\n",
      "loss: 0.402961  [54432/60000]\n",
      "loss: 0.203729  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.268381 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.085799  [   32/60000]\n",
      "loss: 0.042467  [ 3232/60000]\n",
      "loss: 0.122241  [ 6432/60000]\n",
      "loss: 0.331998  [ 9632/60000]\n",
      "loss: 0.106726  [12832/60000]\n",
      "loss: 0.160289  [16032/60000]\n",
      "loss: 0.207146  [19232/60000]\n",
      "loss: 0.271334  [22432/60000]\n",
      "loss: 0.288286  [25632/60000]\n",
      "loss: 0.155949  [28832/60000]\n",
      "loss: 0.208908  [32032/60000]\n",
      "loss: 0.186954  [35232/60000]\n",
      "loss: 0.175455  [38432/60000]\n",
      "loss: 0.062961  [41632/60000]\n",
      "loss: 0.297064  [44832/60000]\n",
      "loss: 0.087594  [48032/60000]\n",
      "loss: 0.205404  [51232/60000]\n",
      "loss: 0.301925  [54432/60000]\n",
      "loss: 0.146099  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.288250 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.336753  [   32/60000]\n",
      "loss: 0.078502  [ 3232/60000]\n",
      "loss: 0.170557  [ 6432/60000]\n",
      "loss: 0.226039  [ 9632/60000]\n",
      "loss: 0.229535  [12832/60000]\n",
      "loss: 0.120241  [16032/60000]\n",
      "loss: 0.097264  [19232/60000]\n",
      "loss: 0.290731  [22432/60000]\n",
      "loss: 0.370098  [25632/60000]\n",
      "loss: 0.107797  [28832/60000]\n",
      "loss: 0.263076  [32032/60000]\n",
      "loss: 0.262098  [35232/60000]\n",
      "loss: 0.101619  [38432/60000]\n",
      "loss: 0.503757  [41632/60000]\n",
      "loss: 0.100044  [44832/60000]\n",
      "loss: 0.321522  [48032/60000]\n",
      "loss: 0.092651  [51232/60000]\n",
      "loss: 0.071728  [54432/60000]\n",
      "loss: 0.098726  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.271255 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.213389  [   32/60000]\n",
      "loss: 0.122507  [ 3232/60000]\n",
      "loss: 0.152720  [ 6432/60000]\n",
      "loss: 0.045119  [ 9632/60000]\n",
      "loss: 0.265572  [12832/60000]\n",
      "loss: 0.064430  [16032/60000]\n",
      "loss: 0.330376  [19232/60000]\n",
      "loss: 0.119238  [22432/60000]\n",
      "loss: 0.237747  [25632/60000]\n",
      "loss: 0.077933  [28832/60000]\n",
      "loss: 0.094297  [32032/60000]\n",
      "loss: 0.116491  [35232/60000]\n",
      "loss: 0.073078  [38432/60000]\n",
      "loss: 0.156822  [41632/60000]\n",
      "loss: 0.013558  [44832/60000]\n",
      "loss: 0.373609  [48032/60000]\n",
      "loss: 0.072466  [51232/60000]\n",
      "loss: 0.101954  [54432/60000]\n",
      "loss: 0.115915  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.269181 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.083314  [   32/60000]\n",
      "loss: 0.083379  [ 3232/60000]\n",
      "loss: 0.218546  [ 6432/60000]\n",
      "loss: 0.239917  [ 9632/60000]\n",
      "loss: 0.093526  [12832/60000]\n",
      "loss: 0.243510  [16032/60000]\n",
      "loss: 0.191309  [19232/60000]\n",
      "loss: 0.307397  [22432/60000]\n",
      "loss: 0.297138  [25632/60000]\n",
      "loss: 0.226345  [28832/60000]\n",
      "loss: 0.426661  [32032/60000]\n",
      "loss: 0.073131  [35232/60000]\n",
      "loss: 0.231893  [38432/60000]\n",
      "loss: 0.069124  [41632/60000]\n",
      "loss: 0.140977  [44832/60000]\n",
      "loss: 0.072451  [48032/60000]\n",
      "loss: 0.032658  [51232/60000]\n",
      "loss: 0.105554  [54432/60000]\n",
      "loss: 0.423128  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.264789 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.076785  [   32/60000]\n",
      "loss: 0.362872  [ 3232/60000]\n",
      "loss: 0.104920  [ 6432/60000]\n",
      "loss: 0.143221  [ 9632/60000]\n",
      "loss: 0.047540  [12832/60000]\n",
      "loss: 0.297315  [16032/60000]\n",
      "loss: 0.254420  [19232/60000]\n",
      "loss: 0.108142  [22432/60000]\n",
      "loss: 0.115501  [25632/60000]\n",
      "loss: 0.235675  [28832/60000]\n",
      "loss: 0.095835  [32032/60000]\n",
      "loss: 0.215923  [35232/60000]\n",
      "loss: 0.103293  [38432/60000]\n",
      "loss: 0.124070  [41632/60000]\n",
      "loss: 0.054354  [44832/60000]\n",
      "loss: 0.120010  [48032/60000]\n",
      "loss: 0.033913  [51232/60000]\n",
      "loss: 0.221964  [54432/60000]\n",
      "loss: 0.313209  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.273460 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.116137  [   32/60000]\n",
      "loss: 0.144907  [ 3232/60000]\n",
      "loss: 0.054080  [ 6432/60000]\n",
      "loss: 0.108117  [ 9632/60000]\n",
      "loss: 0.026924  [12832/60000]\n",
      "loss: 0.156826  [16032/60000]\n",
      "loss: 0.149283  [19232/60000]\n",
      "loss: 0.079609  [22432/60000]\n",
      "loss: 0.407015  [25632/60000]\n",
      "loss: 0.061887  [28832/60000]\n",
      "loss: 0.288837  [32032/60000]\n",
      "loss: 0.156003  [35232/60000]\n",
      "loss: 0.077376  [38432/60000]\n",
      "loss: 0.103021  [41632/60000]\n",
      "loss: 0.275036  [44832/60000]\n",
      "loss: 0.448093  [48032/60000]\n",
      "loss: 0.167446  [51232/60000]\n",
      "loss: 0.138165  [54432/60000]\n",
      "loss: 0.117449  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.281719 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.064813  [   32/60000]\n",
      "loss: 0.056561  [ 3232/60000]\n",
      "loss: 0.278740  [ 6432/60000]\n",
      "loss: 0.162968  [ 9632/60000]\n",
      "loss: 0.147929  [12832/60000]\n",
      "loss: 0.130477  [16032/60000]\n",
      "loss: 0.106490  [19232/60000]\n",
      "loss: 0.073013  [22432/60000]\n",
      "loss: 0.168813  [25632/60000]\n",
      "loss: 0.159673  [28832/60000]\n",
      "loss: 0.062669  [32032/60000]\n",
      "loss: 0.310949  [35232/60000]\n",
      "loss: 0.089147  [38432/60000]\n",
      "loss: 0.153910  [41632/60000]\n",
      "loss: 0.259283  [44832/60000]\n",
      "loss: 0.054658  [48032/60000]\n",
      "loss: 0.106664  [51232/60000]\n",
      "loss: 0.133415  [54432/60000]\n",
      "loss: 0.038434  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.273758 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.163757  [   32/60000]\n",
      "loss: 0.264603  [ 3232/60000]\n",
      "loss: 0.055641  [ 6432/60000]\n",
      "loss: 0.156447  [ 9632/60000]\n",
      "loss: 0.134624  [12832/60000]\n",
      "loss: 0.215383  [16032/60000]\n",
      "loss: 0.227213  [19232/60000]\n",
      "loss: 0.049713  [22432/60000]\n",
      "loss: 0.154122  [25632/60000]\n",
      "loss: 0.164634  [28832/60000]\n",
      "loss: 0.087978  [32032/60000]\n",
      "loss: 0.266975  [35232/60000]\n",
      "loss: 0.190749  [38432/60000]\n",
      "loss: 0.210702  [41632/60000]\n",
      "loss: 0.035906  [44832/60000]\n",
      "loss: 0.311323  [48032/60000]\n",
      "loss: 0.157082  [51232/60000]\n",
      "loss: 0.191425  [54432/60000]\n",
      "loss: 0.034886  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.271316 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.282333  [   32/60000]\n",
      "loss: 0.052107  [ 3232/60000]\n",
      "loss: 0.046348  [ 6432/60000]\n",
      "loss: 0.131490  [ 9632/60000]\n",
      "loss: 0.282842  [12832/60000]\n",
      "loss: 0.164683  [16032/60000]\n",
      "loss: 0.019283  [19232/60000]\n",
      "loss: 0.183405  [22432/60000]\n",
      "loss: 0.059846  [25632/60000]\n",
      "loss: 0.258245  [28832/60000]\n",
      "loss: 0.110348  [32032/60000]\n",
      "loss: 0.053123  [35232/60000]\n",
      "loss: 0.102010  [38432/60000]\n",
      "loss: 0.082390  [41632/60000]\n",
      "loss: 0.192451  [44832/60000]\n",
      "loss: 0.045136  [48032/60000]\n",
      "loss: 0.096203  [51232/60000]\n",
      "loss: 0.156740  [54432/60000]\n",
      "loss: 0.076054  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.285708 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.129791  [   32/60000]\n",
      "loss: 0.128934  [ 3232/60000]\n",
      "loss: 0.202663  [ 6432/60000]\n",
      "loss: 0.127774  [ 9632/60000]\n",
      "loss: 0.117092  [12832/60000]\n",
      "loss: 0.060280  [16032/60000]\n",
      "loss: 0.262862  [19232/60000]\n",
      "loss: 0.141151  [22432/60000]\n",
      "loss: 0.029661  [25632/60000]\n",
      "loss: 0.041690  [28832/60000]\n",
      "loss: 0.109396  [32032/60000]\n",
      "loss: 0.181056  [35232/60000]\n",
      "loss: 0.043905  [38432/60000]\n",
      "loss: 0.115106  [41632/60000]\n",
      "loss: 0.173828  [44832/60000]\n",
      "loss: 0.185891  [48032/60000]\n",
      "loss: 0.123591  [51232/60000]\n",
      "loss: 0.045580  [54432/60000]\n",
      "loss: 0.189186  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.310356 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.333411  [   32/60000]\n",
      "loss: 0.181454  [ 3232/60000]\n",
      "loss: 0.118868  [ 6432/60000]\n",
      "loss: 0.062798  [ 9632/60000]\n",
      "loss: 0.089102  [12832/60000]\n",
      "loss: 0.087384  [16032/60000]\n",
      "loss: 0.086184  [19232/60000]\n",
      "loss: 0.187983  [22432/60000]\n",
      "loss: 0.141127  [25632/60000]\n",
      "loss: 0.030990  [28832/60000]\n",
      "loss: 0.081619  [32032/60000]\n",
      "loss: 0.044894  [35232/60000]\n",
      "loss: 0.141570  [38432/60000]\n",
      "loss: 0.348816  [41632/60000]\n",
      "loss: 0.032417  [44832/60000]\n",
      "loss: 0.050109  [48032/60000]\n",
      "loss: 0.051370  [51232/60000]\n",
      "loss: 0.260288  [54432/60000]\n",
      "loss: 0.064405  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.286695 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.108248  [   32/60000]\n",
      "loss: 0.101390  [ 3232/60000]\n",
      "loss: 0.088946  [ 6432/60000]\n",
      "loss: 0.101217  [ 9632/60000]\n",
      "loss: 0.242060  [12832/60000]\n",
      "loss: 0.115257  [16032/60000]\n",
      "loss: 0.033259  [19232/60000]\n",
      "loss: 0.140951  [22432/60000]\n",
      "loss: 0.091650  [25632/60000]\n",
      "loss: 0.205508  [28832/60000]\n",
      "loss: 0.129199  [32032/60000]\n",
      "loss: 0.112447  [35232/60000]\n",
      "loss: 0.047459  [38432/60000]\n",
      "loss: 0.061371  [41632/60000]\n",
      "loss: 0.179530  [44832/60000]\n",
      "loss: 0.140593  [48032/60000]\n",
      "loss: 0.330671  [51232/60000]\n",
      "loss: 0.082700  [54432/60000]\n",
      "loss: 0.061388  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.308523 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.062349  [   32/60000]\n",
      "loss: 0.053330  [ 3232/60000]\n",
      "loss: 0.149874  [ 6432/60000]\n",
      "loss: 0.043049  [ 9632/60000]\n",
      "loss: 0.069567  [12832/60000]\n",
      "loss: 0.450677  [16032/60000]\n",
      "loss: 0.039248  [19232/60000]\n",
      "loss: 0.234106  [22432/60000]\n",
      "loss: 0.036453  [25632/60000]\n",
      "loss: 0.021804  [28832/60000]\n",
      "loss: 0.254341  [32032/60000]\n",
      "loss: 0.209132  [35232/60000]\n",
      "loss: 0.181633  [38432/60000]\n",
      "loss: 0.087086  [41632/60000]\n",
      "loss: 0.065847  [44832/60000]\n",
      "loss: 0.154232  [48032/60000]\n",
      "loss: 0.024862  [51232/60000]\n",
      "loss: 0.037945  [54432/60000]\n",
      "loss: 0.054166  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.298080 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# your code here\n",
    "\n",
    "import torch.optim.sgd\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_task_2.parameters(), lr=0.01)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "print(1e-1)\n",
    "epochs = 25 # 20 -> 0,91 0,883\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_data_loader, model_task_2, loss_fn, optimizer)\n",
    "    test(test_data_loader, model_task_2, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка архитектуры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "layers_task_2 = []\n",
    "for element in parse_pytorch_model(str(model_task_2)).get(\"layers\", []):\n",
    "    layer_name = element[\"layer\"][\"type\"]\n",
    "    assert \"dropout\" not in layer_name.lower(), \"Do not use Dropout in Task 2!\"\n",
    "    assert \"batchnorm\" not in layer_name.lower(), \"Do not use BatchNorm in Task 2!\"\n",
    "    layers_task_2.append(layer_name)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.9699\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_2 = get_accuracy(model_task_2, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_2:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.9107\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_2 = get_accuracy(model_task_2, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_2:3.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что переобучение присутствует:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_acc_task_2 >= test_acc_task_2, \"Train accuracy must be higher than task accuracy\"\n",
    "assert train_acc_task_2 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
    "assert (\n",
    "    train_acc_task_2 - test_acc_task_2 >= 0.04\n",
    "), \"Test accuracy should be at least 0.04 lower that train.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_2`.\n",
    "\n",
    "Также предполагается, что в переменной `submission_dict` уже содержатся результаты задачи №1. Если их там нет, загрузите их из сохраненного файла в переменную перед запуском следующей ячейки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_tasks_1_and_2.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict.update(\n",
    "    {\n",
    "        \"train_predictions_task_2\": get_predictions(\n",
    "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "        ),\n",
    "        \"test_predictions_task_2\": get_predictions(\n",
    "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "        ),\n",
    "        \"model_task_2\": parse_pytorch_model(str(model_task_2)),\n",
    "    }\n",
    ")\n",
    "\n",
    "with open(\"submission_dict_tasks_1_and_2.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_tasks_1_and_2.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №3: Исправление модели (Return) \n",
    "Все так же работаем с [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). Наконец, ваша задача исправить ~~ошибки прошлого~~ переобучение модели, построенной в задаче 2. Достаточно добиться расхождения между точностью классификации на обучающей и тестовой выборках не превышающего 0.015 (т.е. полутора процентов).\n",
    "\n",
    "Обращаем ваше внимание, архитектура модели в задаче №3 не должна существенно отличаться от задачи №2! Вы можете использовать Batchnorm, Dropout, уменьшить размерность промежуточных представлений, обратиться к аугментации данных, но вы не можете использовать меньшее количество слоёв.\n",
    "\n",
    "__Ваша третья и финальная задача: исправить модель и/или процесс обучения, дабы справиться с переобучением.__\n",
    "\n",
    "Код для обучения модели вы можете переиспользовать. Далее присутствует лишь несколько тестов, которые помогут вам проверить свое решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимание, вам необходимо использовать переменную `model_task_3` для хранение модели во второй задаче. \n",
    "\n",
    "Также код ниже будет обращаться к переменной `layers_task_2`, инициализируйте её, если она не определена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert (\n",
    "    layers_task_2 is not None\n",
    "), \"Initializa layers_task_2 vairable which contains list of layers in task 2 model\"\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=1568, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_task_3 = None\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(1, 1, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1, 2, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*28*28, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_task_3 = NeuralNetwork().to(device)\n",
    "display(model_task_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.190476  [   32/60000]\n",
      "loss: 0.296266  [ 3232/60000]\n",
      "loss: 0.363990  [ 6432/60000]\n",
      "loss: 0.340421  [ 9632/60000]\n",
      "loss: 0.491138  [12832/60000]\n",
      "loss: 0.377474  [16032/60000]\n",
      "loss: 0.327192  [19232/60000]\n",
      "loss: 0.430114  [22432/60000]\n",
      "loss: 0.341196  [25632/60000]\n",
      "loss: 0.136516  [28832/60000]\n",
      "loss: 0.296803  [32032/60000]\n",
      "loss: 0.285123  [35232/60000]\n",
      "loss: 0.328570  [38432/60000]\n",
      "loss: 0.465822  [41632/60000]\n",
      "loss: 0.267658  [44832/60000]\n",
      "loss: 0.448518  [48032/60000]\n",
      "loss: 0.445184  [51232/60000]\n",
      "loss: 0.206094  [54432/60000]\n",
      "loss: 0.437256  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.396474 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "import torch.optim.sgd\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_task_3.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "epochs = 1 # 20 -> 0,91 0,883\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_data_loader, model_task_3, loss_fn, optimizer)\n",
    "    test(test_data_loader, model_task_3, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка архитектуры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "layers_task_3 = []\n",
    "for element in parse_pytorch_model(str(model_task_3)).get(\"layers\", []):\n",
    "    layer_name = element[\"layer\"][\"type\"]\n",
    "    layers_task_3.append(layer_name)\n",
    "\n",
    "#display(layers_task_3)\n",
    "\n",
    "idx = 0\n",
    "for model_3_layer in layers_task_3:\n",
    "    model_2_layer = layers_task_2[idx]\n",
    "    if \"dropout\" not in model_3_layer.lower() and \"batchnorm\" not in model_3_layer.lower():\n",
    "        assert (\n",
    "            model_3_layer == model_2_layer\n",
    "        ), f\"Models in tasks 2 and 3 must share the architecture except for Dropout and BatchNorm!, {model_3_layer}, {model_2_layer}\"\n",
    "        idx += 1\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.87908\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_3 = get_accuracy(model_task_3, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_3:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.8589\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_3 = get_accuracy(model_task_3, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_3:3.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что переобучение присутствует:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Train accuracy must be higher than 0.88",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_acc_task_3 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.88\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain accuracy must be higher than 0.88\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_acc_task_3 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.865\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy must be higher than 0.865\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     train_acc_task_3 \u001b[38;5;241m-\u001b[39m test_acc_task_3 \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.015\u001b[39m\n\u001b[1;32m      5\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy should not be lower that train more than by 0.015, now is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc_task_3\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtest_acc_task_3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Train accuracy must be higher than 0.88"
     ]
    }
   ],
   "source": [
    "assert train_acc_task_3 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
    "assert train_acc_task_3 >= 0.865, \"Test accuracy must be higher than 0.865\"\n",
    "assert (\n",
    "    train_acc_task_3 - test_acc_task_3 <= 0.015\n",
    "), f\"Test accuracy should not be lower that train more than by 0.015, now is {train_acc_task_3 - test_acc_task_3}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_3`.\n",
    "\n",
    "Также предполагается, что в переменной `submission_dict` уже содержатся результаты задач №1 и №2. Если их там нет, загрузите их из сохраненных файлов перед запуском следующей ячейки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict.update(\n",
    "    {\n",
    "        \"train_predictions_task_3\": get_predictions(\n",
    "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "        ),\n",
    "        \"test_predictions_task_3\": get_predictions(\n",
    "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "        ),\n",
    "        \"model_task_3\": parse_pytorch_model(str(model_task_3)),\n",
    "    }\n",
    ")\n",
    "\n",
    "with open(\"submission_dict_final.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_final.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xai8JL3tgSq_"
   },
   "source": [
    "### Сдача задания\n",
    "Сдайте сгенерированные файлы в соответствующие задачи в соревновании, а именно:\n",
    "* `submission_dict_tasks_1_and_2.json` в задачу Initiation\n",
    "* `submission_dict_final.json` в задачу Return.\n",
    "\n",
    "\n",
    "`submission_dict_task_1.json` сдавать не нужно, он уже был сдан ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtWnYAN_gSrA"
   },
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlhw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
